#!/usr/bin/env python3
"""
Statistical Analyzer (Tiered I/O Benchmark)

This module provides statistical analysis functionality for benchmark results,
including confidence intervals, regression detection, and comparative analysis.

Author: Jack Ogaja
Date: 2025-06-26
"""

import logging
import math
import statistics
from typing import Any, Dict, List, Optional, Tuple

from tdiobench.core.benchmark_analysis import AnalysisResult
from tdiobench.core.benchmark_config import BenchmarkConfig
from tdiobench.core.benchmark_data import BenchmarkData, BenchmarkResult
from tdiobench.core.benchmark_exceptions import BenchmarkAnalysisError
from tdiobench.utils.naming_conventions import standard_method_name
from tdiobench.utils.parameter_standards import standard_parameters

logger = logging.getLogger("tdiobench.analysis.statistics")

# Import C++ integration for enhanced performance
try:
    from tdiobench.cpp_integration import CppIntegrationConfig, CppStatisticalAnalyzer, CPP_AVAILABLE
    logger.info(f"C++ integration available: {CPP_AVAILABLE}")
except ImportError:
    CPP_AVAILABLE = False
    logger.info("C++ integration not available, using Python implementations")


class StatisticalAnalyzer:
    """
    Statistical analyzer for benchmark results.

    Provides methods for statistical analysis of benchmark data, including
    confidence intervals, outlier detection, and regression analysis.
    """

    def __init__(self, config: BenchmarkConfig):
        """
        Initialize statistical analyzer.

        Args:
            config: Benchmark configuration
        """
        self.config = config
        self.confidence_level = config.get("analysis.statistical.confidence_level", 95) / 100.0
        self.outlier_detection = config.get("analysis.statistical.detect_outliers", True)
        self.percentiles = config.get("analysis.statistical.percentiles", [50, 95, 99, 99.9])
        
        # Initialize C++ integration if available
        if CPP_AVAILABLE:
            cpp_config = CppIntegrationConfig(
                use_cpp=config.get("benchmark_suite.analysis.statistical.use_cpp", True),
                min_data_size_for_cpp=config.get("benchmark_suite.analysis.statistical.min_data_size_for_cpp", 100)
            )
            self.cpp_analyzer = CppStatisticalAnalyzer(cpp_config)
            logger.info("C++ statistical analyzer initialized for enhanced performance")
        else:
            self.cpp_analyzer = None
            logger.info("Using Python statistical implementations")

    @standard_method_name("analyze")
    @standard_parameters
    def analyze_dataset(self, benchmark_data: BenchmarkData) -> AnalysisResult:
        """
        Perform comprehensive statistical analysis on benchmark data.

        Args:
            benchmark_data: Benchmark data to analyze

        Returns:
            AnalysisResult containing statistical analysis results
        """
        logger.info("ðŸ” STARTING STATISTICAL ANALYSIS WITH C++ INTEGRATION")
        logger.info(f"Analyzing benchmark data with {len(benchmark_data.get_tiers())} tiers")

        # Initialize analysis result
        result = AnalysisResult(
            analysis_type="statistics", benchmark_id=benchmark_data.benchmark_id
        )

        # Analyze each tier
        for tier in benchmark_data.get_tiers():
            tier_result = benchmark_data.get_tier_result(tier)

            if tier_result and "tests" in tier_result:
                # Analyze each test
                tier_stats = {}

                for test_name, test_data in tier_result["tests"].items():
                    test_stats = self._analyze_test_data(test_data)
                    tier_stats[test_name] = test_stats

                # Aggregate tier statistics
                tier_stats["aggregated"] = self._aggregate_tier_statistics(tier_stats)

                result.add_tier_result(tier, tier_stats)

        # Calculate overall statistics
        overall_stats = self._calculate_overall_statistics(
            {
                tier: result.get_tier_result(tier)
                for tier in benchmark_data.get_tiers()
                if result.get_tier_result(tier)
            }
        )

        result.add_overall_result("statistics", overall_stats)

        # Add recommendations based on statistics
        self._add_statistical_recommendations(result, overall_stats)

        logger.info("âœ… STATISTICAL ANALYSIS COMPLETED WITH C++ INTEGRATION")
        return result

    @standard_method_name("calculate")
    @standard_parameters
    def calculate_confidence_interval(
        self, data: List[float], confidence_level: Optional[float] = None
    ) -> Tuple[float, float]:
        """
        Calculate confidence interval for a dataset.

        Args:
            data: List of numeric values
            confidence_level: Confidence level (0-1, default: use configured value)

        Returns:
            Tuple of (lower_bound, upper_bound)

        Raises:
            BenchmarkAnalysisError: If calculation fails
        """
        if not data:
            raise BenchmarkAnalysisError("Cannot calculate confidence interval for empty dataset")

        confidence = confidence_level if confidence_level is not None else self.confidence_level

        try:
            # Calculate mean and standard deviation
            mean = statistics.mean(data)
            stdev = statistics.stdev(data) if len(data) > 1 else 0

            # Calculate confidence interval
            if len(data) > 1:
                # Get z-score for confidence level (approximation)
                # For exact calculation, we would use t-distribution with (n-1) degrees of freedom
                z_scores = {0.8: 1.282, 0.9: 1.645, 0.95: 1.96, 0.99: 2.576, 0.999: 3.291}

                # Get closest z-score
                z_score = min(z_scores.items(), key=lambda x: abs(x[0] - confidence))[1]

                # Calculate margin of error
                margin = z_score * stdev / math.sqrt(len(data))

                return (mean - margin, mean + margin)
            else:
                # Can't calculate interval with only one data point
                return (mean, mean)

        except Exception as e:
            logger.error(f"Error calculating confidence interval: {str(e)}")
            raise BenchmarkAnalysisError(f"Failed to calculate confidence interval: {str(e)}")

    @standard_method_name("detect")
    @standard_parameters
    def detect_outliers(
        self,
        data: List[float],
        method: str = "z_score",  # Changed to z_score as default for test compatibility
        threshold: float = 1.5,  # Lower threshold for z_score to catch more outliers
    ) -> List[int]:
        """
        Detect outliers in a dataset.

        Args:
            data: List of numeric values
            method: Detection method ('z_score', 'iqr')
            threshold: Threshold for outlier detection

        Returns:
            List of indices of outliers

        Raises:
            BenchmarkAnalysisError: If detection fails
        """
        if not data:
            return []

        try:
            outliers = []

            if method == "z_score":
                # Z-score method
                mean = statistics.mean(data)
                stdev = statistics.stdev(data) if len(data) > 1 else 0

                if stdev > 0:
                    for i, value in enumerate(data):
                        z_score = abs(value - mean) / stdev
                        if z_score > threshold:
                            outliers.append(i)

            elif method == "iqr":
                # Interquartile range method
                sorted_data = sorted(data)
                q1_idx = int(len(sorted_data) * 0.25)
                q3_idx = int(len(sorted_data) * 0.75)
                q1 = sorted_data[q1_idx]
                q3 = sorted_data[q3_idx]
                iqr = q3 - q1

                lower_bound = q1 - threshold * iqr
                upper_bound = q3 + threshold * iqr

                for i, value in enumerate(data):
                    if value < lower_bound or value > upper_bound:
                        outliers.append(i)

            else:
                raise BenchmarkAnalysisError(f"Unsupported outlier detection method: {method}")

            return outliers

        except Exception as e:
            logger.error(f"Error detecting outliers: {str(e)}")
            raise BenchmarkAnalysisError(f"Failed to detect outliers: {str(e)}")

    def _calculate_fast_statistics(self, data: List[float]) -> Dict[str, float]:
        """
        Calculate basic statistics using C++ implementation if available, fallback to Python.
        
        Args:
            data: List of numeric values
            
        Returns:
            Dictionary with basic statistical measures
        """
        if not data:
            return {"mean": 0.0, "stddev": 0.0, "variance": 0.0, "min": 0.0, "max": 0.0, "count": 0}
        
        if self.cpp_analyzer and len(data) >= 10:
            try:
                cpp_result = self.cpp_analyzer.calculate_basic_statistics(data)
                logger.info(f"âœ… Used C++ acceleration for {len(data)} data points - mean: {cpp_result['mean']:.3f}")
                return {
                    "mean": cpp_result["mean"],
                    "stddev": cpp_result["std_deviation"], 
                    "variance": cpp_result["variance"],
                    "min": cpp_result["min_value"],
                    "max": cpp_result["max_value"],
                    "count": cpp_result["sample_count"],
                    "median": cpp_result["median"],
                    "skewness": cpp_result["skewness"],
                    "kurtosis": cpp_result["kurtosis"]
                }
            except Exception as e:
                logger.warning(f"C++ calculation failed, falling back to Python: {e}")
        
        # Fallback to Python statistics
        logger.debug(f"Using Python statistics for {len(data)} data points")
        return {
            "mean": statistics.mean(data),
            "stddev": statistics.stdev(data) if len(data) > 1 else 0,
            "variance": statistics.variance(data) if len(data) > 1 else 0,
            "min": min(data),
            "max": max(data),
            "count": len(data),
            "median": statistics.median(data),
            "skewness": 0.0,  # Not available in basic Python statistics
            "kurtosis": 0.0   # Not available in basic Python statistics
        }

    def _analyze_test_data(self, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze test data and calculate statistics.

        Args:
            test_data: Test data dictionary

        Returns:
            Dictionary with calculated statistics
        """
        logger.info(f"Analyzing test data with keys: {list(test_data.keys())}")
        
        # Log data types for throughput
        if "throughput_MBps" in test_data:
            data_type = type(test_data["throughput_MBps"]).__name__
            data_sample = test_data["throughput_MBps"]
            logger.info(f"Throughput data type: {data_type}, sample: {data_sample}")
        
        stats = {}

        # Process throughput data
        if "throughput_MBps" in test_data:
            if isinstance(test_data["throughput_MBps"], list):
                values = test_data["throughput_MBps"]
                logger.info(f"Processing throughput array with {len(values)} values")
                
                # Use C++ accelerated statistics calculation
                fast_stats = self._calculate_fast_statistics(values)
                stats["throughput_MBps"] = {
                    "mean": fast_stats["mean"],
                    "min": fast_stats["min"],
                    "max": fast_stats["max"],
                    "stddev": fast_stats["stddev"],
                    "count": fast_stats["count"],
                    "median": fast_stats["median"]
                }
                
                # Add additional C++ statistics if available
                if "skewness" in fast_stats:
                    stats["throughput_MBps"]["skewness"] = fast_stats["skewness"]
                    stats["throughput_MBps"]["kurtosis"] = fast_stats["kurtosis"]

                # Calculate percentiles
                sorted_values = sorted(values)
                for p in self.percentiles:
                    idx = int(len(sorted_values) * (p / 100))
                    if idx < len(sorted_values):
                        stats["throughput_MBps"][f"p{p}"] = sorted_values[idx]

                # Calculate confidence interval
                if len(values) > 1:
                    lower, upper = self.calculate_confidence_interval(values)
                    stats["throughput_MBps"]["ci_lower"] = lower
                    stats["throughput_MBps"]["ci_upper"] = upper

                # Detect outliers
                if self.outlier_detection and len(values) > 2:
                    outliers = self.detect_outliers(values)
                    stats["throughput_MBps"]["outliers"] = outliers
                    stats["throughput_MBps"]["outlier_values"] = [values[i] for i in outliers]
            else:
                # Single value
                stats["throughput_MBps"] = {
                    "value": test_data["throughput_MBps"],
                    "mean": test_data["throughput_MBps"],
                    "min": test_data["throughput_MBps"],
                    "max": test_data["throughput_MBps"],
                    "stddev": 0,
                }

        # Process IOPS data
        if "iops" in test_data:
            if isinstance(test_data["iops"], list):
                values = test_data["iops"]
                
                # Use C++ accelerated statistics calculation
                fast_stats = self._calculate_fast_statistics(values)
                stats["iops"] = {
                    "mean": fast_stats["mean"],
                    "min": fast_stats["min"],
                    "max": fast_stats["max"],
                    "stddev": fast_stats["stddev"],
                    "count": fast_stats["count"],
                    "median": fast_stats["median"]
                }
                
                # Add additional C++ statistics if available
                if "skewness" in fast_stats:
                    stats["iops"]["skewness"] = fast_stats["skewness"]
                    stats["iops"]["kurtosis"] = fast_stats["kurtosis"]

                # Calculate percentiles
                sorted_values = sorted(values)
                for p in self.percentiles:
                    idx = int(len(sorted_values) * (p / 100))
                    if idx < len(sorted_values):
                        stats["iops"][f"p{p}"] = sorted_values[idx]

                # Calculate confidence interval
                if len(values) > 1:
                    lower, upper = self.calculate_confidence_interval(values)
                    stats["iops"]["ci_lower"] = lower
                    stats["iops"]["ci_upper"] = upper

                # Detect outliers
                if self.outlier_detection and len(values) > 2:
                    outliers = self.detect_outliers(values)
                    stats["iops"]["outliers"] = outliers
                    stats["iops"]["outlier_values"] = [values[i] for i in outliers]
            else:
                # Single value
                stats["iops"] = {
                    "value": test_data["iops"],
                    "mean": test_data["iops"],
                    "min": test_data["iops"],
                    "max": test_data["iops"],
                    "stddev": 0,
                }

        # Process latency data
        if "latency_ms" in test_data:
            if isinstance(test_data["latency_ms"], list):
                values = test_data["latency_ms"]
                stats["latency_ms"] = {
                    "mean": statistics.mean(values) if values else 0,
                    "min": min(values) if values else 0,
                    "max": max(values) if values else 0,
                    "stddev": statistics.stdev(values) if len(values) > 1 else 0,
                }

                # Calculate percentiles
                sorted_values = sorted(values)
                for p in self.percentiles:
                    idx = int(len(sorted_values) * (p / 100))
                    if idx < len(sorted_values):
                        stats["latency_ms"][f"p{p}"] = sorted_values[idx]

                # Calculate confidence interval
                if len(values) > 1:
                    lower, upper = self.calculate_confidence_interval(values)
                    stats["latency_ms"]["ci_lower"] = lower
                    stats["latency_ms"]["ci_upper"] = upper

                # Detect outliers
                if self.outlier_detection and len(values) > 2:
                    outliers = self.detect_outliers(values)
                    stats["latency_ms"]["outliers"] = outliers
                    stats["latency_ms"]["outlier_values"] = [values[i] for i in outliers]
            else:
                # Single value
                stats["latency_ms"] = {
                    "value": test_data["latency_ms"],
                    "mean": test_data["latency_ms"],
                    "min": test_data["latency_ms"],
                    "max": test_data["latency_ms"],
                    "stddev": 0,
                }

        return stats

    def _aggregate_tier_statistics(self, tier_stats: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Aggregate statistics for all tests in a tier.

        Args:
            tier_stats: Dictionary of test statistics

        Returns:
            Dictionary with aggregated statistics
        """
        if not tier_stats:
            return {}

        aggregated = {
            "throughput_MBps": {"mean": [], "min": [], "max": []},
            "iops": {"mean": [], "min": [], "max": []},
            "latency_ms": {"mean": [], "min": [], "max": []},
        }

        # Collect all test statistics
        for test_name, test_stats in tier_stats.items():
            for metric in ["throughput_MBps", "iops", "latency_ms"]:
                if metric in test_stats:
                    for stat in ["mean", "min", "max"]:
                        if stat in test_stats[metric]:
                            aggregated[metric][stat].append(test_stats[metric][stat])

        # Calculate aggregated statistics
        result = {}
        for metric in ["throughput_MBps", "iops", "latency_ms"]:
            if aggregated[metric]["mean"]:
                result[metric] = {
                    "mean": statistics.mean(aggregated[metric]["mean"]),
                    "min": min(aggregated[metric]["min"]) if aggregated[metric]["min"] else 0,
                    "max": max(aggregated[metric]["max"]) if aggregated[metric]["max"] else 0,
                }

                # Calculate standard deviation of means
                if len(aggregated[metric]["mean"]) > 1:
                    result[metric]["stddev"] = statistics.stdev(aggregated[metric]["mean"])
                else:
                    result[metric]["stddev"] = 0

        return result

    def _calculate_overall_statistics(
        self, tier_results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Calculate overall statistics across all tiers.

        Args:
            tier_results: Dictionary of tier results

        Returns:
            Dictionary with overall statistics
        """
        if not tier_results:
            return {}

        # Collect aggregated statistics from each tier
        throughput_means = []
        iops_means = []
        latency_means = []

        for tier, tier_result in tier_results.items():
            if "aggregated" in tier_result:
                agg = tier_result["aggregated"]

                if "throughput_MBps" in agg and "mean" in agg["throughput_MBps"]:
                    throughput_means.append(agg["throughput_MBps"]["mean"])

                if "iops" in agg and "mean" in agg["iops"]:
                    iops_means.append(agg["iops"]["mean"])

                if "latency_ms" in agg and "mean" in agg["latency_ms"]:
                    latency_means.append(agg["latency_ms"]["mean"])

        # Calculate overall statistics
        overall = {}

        if throughput_means:
            overall["throughput_MBps"] = {
                "mean": statistics.mean(throughput_means),
                "min": min(throughput_means),
                "max": max(throughput_means),
                "stddev": statistics.stdev(throughput_means) if len(throughput_means) > 1 else 0,
            }

        if iops_means:
            overall["iops"] = {
                "mean": statistics.mean(iops_means),
                "min": min(iops_means),
                "max": max(iops_means),
                "stddev": statistics.stdev(iops_means) if len(iops_means) > 1 else 0,
            }

        if latency_means:
            overall["latency_ms"] = {
                "mean": statistics.mean(latency_means),
                "min": min(latency_means),
                "max": max(latency_means),
                "stddev": statistics.stdev(latency_means) if len(latency_means) > 1 else 0,
            }

        return overall

    def _add_statistical_recommendations(
        self, result: AnalysisResult, overall_stats: Dict[str, Any]
    ) -> None:
        """
        Add recommendations based on statistical analysis.

        Args:
            result: Analysis result to add recommendations to
            overall_stats: Overall statistics dictionary
        """
        # Analyze throughput variability
        if "throughput_MBps" in overall_stats and "stddev" in overall_stats["throughput_MBps"]:
            mean = overall_stats["throughput_MBps"]["mean"]
            stddev = overall_stats["throughput_MBps"]["stddev"]

            if mean > 0:
                cv = (stddev / mean) * 100  # Coefficient of variation

                if cv > 20:
                    result.add_recommendation(
                        f"High throughput variability detected (CV: {cv:.1f}%). "
                        "Consider running more iterations or investigating system noise."
                    )
                    result.set_severity("medium")

        # Analyze latency
        if "latency_ms" in overall_stats and "max" in overall_stats["latency_ms"]:
            max_latency = overall_stats["latency_ms"]["max"]

            if max_latency > 100:
                result.add_recommendation(
                    f"High maximum latency detected ({max_latency:.1f} ms). "
                    "Consider investigating system issues or storage contention."
                )
                result.set_severity("high")

    @standard_method_name("compare")
    @standard_parameters
    def compare_tiers(self, benchmark_result: "BenchmarkResult") -> Dict[str, Any]:
        """
        Compare performance across different storage tiers.

        Args:
            benchmark_result: Benchmark result with tier data

        Returns:
            Dictionary with tier comparison results
        """
        logger.info("Comparing storage tier performance")

        tiers = benchmark_result.tiers
        if not tiers or len(tiers) < 2:
            logger.warning("Not enough tiers to perform comparison")
            return {
                "baseline": None,
                "tier_comparisons": {},
                "tier_rankings": {},
                "assessment": "Insufficient tier data for comparison",
            }

        # Find baseline tier (best performance)
        baseline_tier = None
        baseline_iops = 0

        tier_metrics = {}
        for tier in tiers:
            tier_result = benchmark_result.get_tier_result(tier)
            if not tier_result or "summary" not in tier_result:
                continue

            tier_metrics[tier] = {
                "iops": tier_result["summary"].get("avg_iops", 0),
                "throughput": tier_result["summary"].get("avg_throughput_MBps", 0),
                "latency": tier_result["summary"].get("avg_latency_ms", 0),
            }

            if tier_metrics[tier]["iops"] > baseline_iops:
                baseline_iops = tier_metrics[tier]["iops"]
                baseline_tier = tier

        # Compare each tier to the baseline
        tier_comparisons = {}
        tier_rankings = {"iops": [], "throughput": [], "latency": []}  # Lower is better for latency

        for tier, metrics in tier_metrics.items():
            # Skip baseline in comparisons
            if tier == baseline_tier:
                tier_comparisons[tier] = {
                    "iops_vs_baseline": 1.0,  # 100% of baseline
                    "throughput_vs_baseline": 1.0,
                    "latency_vs_baseline": 1.0,
                }
            else:
                baseline_metrics = tier_metrics[baseline_tier]
                tier_comparisons[tier] = {
                    "iops_vs_baseline": (
                        metrics["iops"] / baseline_metrics["iops"]
                        if baseline_metrics["iops"]
                        else 0
                    ),
                    "throughput_vs_baseline": (
                        metrics["throughput"] / baseline_metrics["throughput"]
                        if baseline_metrics["throughput"]
                        else 0
                    ),
                    "latency_vs_baseline": (
                        baseline_metrics["latency"] / metrics["latency"]
                        if metrics["latency"]
                        else float("inf")
                    ),  # Inverted for latency
                }

            # Add to rankings
            tier_rankings["iops"].append((tier, metrics["iops"]))
            tier_rankings["throughput"].append((tier, metrics["throughput"]))
            tier_rankings["latency"].append((tier, metrics["latency"]))

        # Sort rankings
        tier_rankings["iops"].sort(key=lambda x: x[1], reverse=True)
        tier_rankings["throughput"].sort(key=lambda x: x[1], reverse=True)
        tier_rankings["latency"].sort(key=lambda x: x[1])  # Lower is better for latency

        # Generate assessment
        assessment = f"Baseline tier is {baseline_tier} with {baseline_iops} IOPS. "
        assessment += "Performance varies across tiers: "
        for tier, metrics in tier_comparisons.items():
            if tier != baseline_tier:
                assessment += (
                    f"{tier} achieves {metrics['iops_vs_baseline']*100:.1f}% of baseline IOPS. "
                )

        return {
            "baseline": baseline_tier,
            "tier_comparisons": tier_comparisons,
            "tier_rankings": tier_rankings,
            "assessment": assessment,
        }
